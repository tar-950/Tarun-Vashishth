{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Using cached https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.4.1)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.10-cp37-none-any.whl size=184496 sha256=9291a28f93d09d70d8b42c2ba17e0bae1a0f98c2fdbab6ba71c354e4d6c8df9d\n",
      "  Stored in directory: C:\\Users\\KailashChandraOli\\AppData\\Local\\pip\\Cache\\wheels\\18\\0b\\a0\\1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.10\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Programme\n",
      "6 files identified\n",
      "Reading File resumes\\atul sharma.pdf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-47c506e82714>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"-v\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-47c506e82714>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputString\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'extension'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fileName'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import nltk, os, subprocess, code, glob, re, traceback, sys, inspect\n",
    "from time import perf_counter, sleep\n",
    "from pprint import pprint\n",
    "import json\n",
    "import zipfile\n",
    "# import ner\n",
    "from convertPDFToText import convertPDFToText\n",
    "from convertDocxToText import convertDocxToText\n",
    "from io import StringIO\n",
    "#from convertRtfToText import convertRtfToText\n",
    "\n",
    "\n",
    "\n",
    "class exportToCSV:\n",
    "    def __init__(self, fileName='resultsCSV.txt', resetFile=False):\n",
    "        headers = ['FILE NAME',\n",
    "               'NAME',\n",
    "               'EMAIL1', 'EMAIL2', 'EMAIL3', 'EMAIL4',\n",
    "               'PHONE1', 'PHONE2', 'PHONE3', 'PHONE4',\n",
    "               'INSTITUTES1','YEARS1',\n",
    "               'INSTITUTES2','YEARS2',\n",
    "               'INSTITUTES3','YEARS3',\n",
    "               'INSTITUTES4','YEARS4',\n",
    "               'INSTITUTES5','YEARS5',\n",
    "               'EXPERIENCE',\n",
    "               'DEGREES',\n",
    "               ]\n",
    "        if not os.path.isfile(fileName) or resetFile:\n",
    "            # Will create/reset the file as per the evaluation of above condition\n",
    "            fOut = open(fileName, 'w')\n",
    "            fOut.close()\n",
    "        fIn = open(fileName) ########### Open file if file already present\n",
    "        inString = fIn.read()\n",
    "        fIn.close()\n",
    "        if len(inString) <= 0: ######### If File already exsists but is empty, it adds the header\n",
    "            fOut = open(fileName, 'w')\n",
    "            fOut.write(','.join(headers)+'\\n')\n",
    "            fOut.close()\n",
    "\n",
    "    def write(self, infoDict):\n",
    "        fOut = open('resultsCSV.txt', 'a+')\n",
    "        # Individual elements are dictionaries\n",
    "        writeString = ''\n",
    "        try:\n",
    "            writeString += str(infoDict['fileName']) + ','\n",
    "            writeString += str(infoDict['name']) + ','\n",
    "            \n",
    "            if infoDict['email']:\n",
    "                writeString += str(','.join(infoDict['email'][:4])) + ','\n",
    "            if len(infoDict['email']) < 4:\n",
    "                writeString += ','*(4-len(infoDict['email']))\n",
    "            if infoDict['phone']:\n",
    "                writeString += str(','.join(infoDict['phone'][:4])) + ','\n",
    "            if len(infoDict['phone']) < 4:\n",
    "                writeString += ','*(4-len(infoDict['phone']))            \n",
    "            writeString += str(infoDict['%sinstitute'%'c\\\\.?a'])+\",\"\n",
    "            writeString +=str(infoDict['%syear'%'c\\\\.?a'])+\",\"\n",
    "            writeString += str(infoDict['%sinstitute'%'b\\\\.?com'])+\",\"\n",
    "            writeString +=str(infoDict['%syear'%'b\\\\.?com'])+\",\"\n",
    "            writeString += str(infoDict['%sinstitute'%'icwa'])+\",\"\n",
    "            writeString +=str(infoDict['%syear'%'icwa'])+\",\"\n",
    "            writeString += str(infoDict['%sinstitute'%'m\\\\.?com'])+\",\"\n",
    "            writeString +=str(infoDict['%syear'%'m\\\\.?com'])+\",\"\n",
    "            writeString += str(infoDict['%sinstitute'%'mba'])+\",\"\n",
    "            writeString +=str(infoDict['%syear'%'mba'])+\",\"\n",
    "            writeString += str(infoDict['experience']) + ','\n",
    "            writeString += str(infoDict['degree']) + '\\n' # For the remaining elements\n",
    "            fOut.write(writeString)\n",
    "        except:\n",
    "            fOut.write('FAILED_TO_WRITE\\n')\n",
    "        fOut.close()\n",
    "\n",
    "\n",
    "    \n",
    "class Parse():\n",
    "    # List (of dictionaries) that will store all of the values\n",
    "    # For processing purposes\n",
    "    information=[]\n",
    "    inputString = ''\n",
    "    tokens = []\n",
    "    lines = []\n",
    "    sentences = []\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        print('Starting Programme')\n",
    "        fields = [\"name\", \"address\", \"email\", \"phone\", \"mobile\", \"telephone\", \"residence status\",\"experience\",\"degree\",\"cainstitute\",\"cayear\",\"caline\",\"b.cominstitute\",\"b.comyear\",\"b.comline\",\"icwainstitue\",\"icwayear\",\"icwaline\",\"m.cominstitute\",\"m.comyear\",\"m.comline\",\"mbainstitute\",\"mbayear\",\"mbaline\"]\n",
    "\n",
    "        # Glob module matches certain patterns\n",
    "        doc_files = glob.glob(\"resumes/*.doc\")\n",
    "        docx_files = glob.glob(\"resumes/*.docx\")\n",
    "        pdf_files = glob.glob(\"resumes/*.pdf\")\n",
    "        rtf_files = glob.glob(\"resumes/*.rtf\")\n",
    "        text_files = glob.glob(\"resumes/*.txt\")\n",
    "\n",
    "        files = set(doc_files + docx_files + pdf_files + rtf_files + text_files)\n",
    "        files = list(files)\n",
    "        print (\"%d files identified\" %len(files))\n",
    " \n",
    "        for f in files:\n",
    "            print(\"Reading File %s\"%f)\n",
    "            # info is a dictionary that stores all the data obtained from parsing\n",
    "            info = {}\n",
    "\n",
    "            self.inputString, info['extension'] = self.readFile(f)           \n",
    "            info['fileName'] = f\n",
    "\n",
    "            self.tokenize(self.inputString)\n",
    "\n",
    "            self.getEmail(self.inputString, info)\n",
    "\n",
    "            self.getPhone(self.inputString, info)\n",
    "\n",
    "            self.getName(self.inputString, info)\n",
    "\n",
    "            self.Qualification(self.inputString,info)\n",
    "\n",
    "            self.getExperience(self.inputString,info,debug=False)\n",
    "\n",
    "            csv=exportToCSV()\n",
    "            csv.write(info)\n",
    "            self.information.append(info)\n",
    "            print (info)\n",
    "\n",
    "        \n",
    "\n",
    "    def readFile(self, fileName):\n",
    "        '''\n",
    "        Read a file given its name as a string.\n",
    "        Modules required: os\n",
    "        UNIX packages required: antiword, ps2ascii\n",
    "        '''\n",
    "        extension = fileName.split(\".\")[-1]\n",
    "        if extension == \"txt\":\n",
    "            f = open(fileName, 'r')\n",
    "            string = f.read()\n",
    "            f.close() \n",
    "            return string, extension\n",
    "        elif extension == \"doc\":\n",
    "            # Run a shell command and store the output as a string\n",
    "            # Antiword is used for extracting data out of Word docs. Does not work with docx, pdf etc.\n",
    "            return subprocess.Popen(['antiword', fileName], stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()[0], extension\n",
    "        elif extension == \"docx\":\n",
    "            try:\n",
    "                return convertDocxToText(fileName), extension\n",
    "            except:\n",
    "                return ''\n",
    "                pass\n",
    "        #elif extension == \"rtf\":\n",
    "        #    try:\n",
    "        #        return convertRtfToText(fileName), extension\n",
    "        #    except:\n",
    "        #        return ''\n",
    "        #        pass\n",
    "        elif extension == \"pdf\":\n",
    "            # ps2ascii converst pdf to ascii text\n",
    "            # May have a potential formatting loss for unicode characters\n",
    "            # return os.system((\"ps2ascii %s\") (fileName))\n",
    "            try:\n",
    "                return convertPDFToText(fileName), extension\n",
    "            except:\n",
    "                return ''\n",
    "                pass\n",
    "        else:\n",
    "            print ('Unsupported format')\n",
    "            return '', ''\n",
    "\n",
    "    def preprocess(self, document):\n",
    "        '''\n",
    "        Information Extraction: Preprocess a document with the necessary POS tagging.\n",
    "        Returns three lists, one with tokens, one with POS tagged lines, one with POS tagged sentences.\n",
    "        Modules required: nltk\n",
    "        '''\n",
    "        try:\n",
    "            # Try to get rid of special characters\n",
    "            try:\n",
    "                document = document.decode('ascii', 'ignore')\n",
    "            except:\n",
    "                document = document.encode('ascii', 'ignore')\n",
    "            # Newlines are one element of structure in the data\n",
    "            # Helps limit the context and breaks up the data as is intended in resumes - i.e., into points\n",
    "            lines = [el.strip() for el in document.split(\"\\n\") if len(el) > 0]  # Splitting on the basis of newlines \n",
    "            lines = [nltk.word_tokenize(el) for el in lines]    # Tokenize the individual lines\n",
    "            lines = [nltk.pos_tag(el) for el in lines]  # Tag them\n",
    "            # Below approach is slightly different because it splits sentences not just on the basis of newlines, but also full stops \n",
    "            # - (barring abbreviations etc.)\n",
    "            # But it fails miserably at predicting names, so currently using it only for tokenization of the whole document\n",
    "            sentences = nltk.sent_tokenize(document)    # Split/Tokenize into sentences (List of strings)\n",
    "            sentences = [nltk.word_tokenize(sent) for sent in sentences]    # Split/Tokenize sentences into words (List of lists of strings)\n",
    "            tokens = sentences\n",
    "            sentences = [nltk.pos_tag(sent) for sent in sentences]    # Tag the tokens - list of lists of tuples - each tuple is (<word>, <tag>)\n",
    "            # Next 4 lines convert tokens from a list of list of strings to a list of strings; basically stitches them together\n",
    "            dummy = []\n",
    "            for el in tokens:\n",
    "                dummy += el\n",
    "            tokens = dummy\n",
    "            # tokens - words extracted from the doc, lines - split only based on newlines (may have more than one sentence)\n",
    "            # sentences - split on the basis of rules of grammar\n",
    "            return tokens, lines, sentences\n",
    "        except Exception as e:\n",
    "            print (e) \n",
    "\n",
    "    def tokenize(self, inputString):\n",
    "        try:\n",
    "            self.tokens, self.lines, self.sentences = self.preprocess(inputString)\n",
    "            return self.tokens, self.lines, self.sentences\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "\n",
    "    def getEmail(self, inputString, infoDict, debug=False): \n",
    "        '''\n",
    "        Given an input string, returns possible matches for emails. Uses regular expression based matching.\n",
    "        Needs an input string, a dictionary where values are being stored, and an optional parameter for debugging.\n",
    "        Modules required: time.perf_counter from time, code.\n",
    "        '''\n",
    "\n",
    "        email = None\n",
    "        try:\n",
    "            pattern = re.compile(r'\\S*@\\S*')\n",
    "            matches = pattern.findall(inputString) # Gets all email addresses as a list\n",
    "            email = matches\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "\n",
    "        infoDict['email'] = email\n",
    "\n",
    "        if debug:\n",
    "            print (\"\\n\", pprint(infoDict), \"\\n\")\n",
    "            code.interact(local=locals())\n",
    "        return email\n",
    "\n",
    "    def getPhone(self, inputString, infoDict, debug=False):\n",
    "        '''\n",
    "        Given an input string, returns possible matches for phone numbers. Uses regular expression based matching.\n",
    "        Needs an input string, a dictionary where values are being stored, and an optional parameter for debugging.\n",
    "        Modules required: time.perf_counter from time, code.\n",
    "        '''\n",
    "\n",
    "        number = None\n",
    "        try:\n",
    "            pattern = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "                # Understanding the above regex\n",
    "                # +91 or (91) -> [+(]? \\d+ -?\n",
    "                # Metacharacters have to be escaped with \\ outside of character classes; inside only hyphen has to be escaped\n",
    "                # hyphen has to be escaped inside the character class if you're not incidication a range\n",
    "                # General number formats are 123 456 7890 or 12345 67890 or 1234567890 or 123-456-7890, hence 3 or more digits\n",
    "                # Amendment to above - some also have (0000) 00 00 00 kind of format\n",
    "                # \\s* is any whitespace character - careful, use [ \\t\\r\\f\\v]* instead since newlines are trouble\n",
    "            match = pattern.findall(inputString)\n",
    "            # match = [re.sub(r'\\s', '', el) for el in match]\n",
    "                # Get rid of random whitespaces - helps with getting rid of 6 digits or fewer (e.g. pin codes) strings\n",
    "            # substitute the characters we don't want just for the purpose of checking\n",
    "            match = [re.sub(r'[,.]', '', el) for el in match if len(re.sub(r'[()\\-.,\\s+]', '', el))>6]\n",
    "                # Taking care of years, eg. 2001-2004 etc.\n",
    "            match = [re.sub(r'\\D$', '', el).strip() for el in match]\n",
    "                # $ matches end of string. This takes care of random trailing non-digit characters. \\D is non-digit characters\n",
    "            match = [el for el in match if len(re.sub(r'\\D','',el)) <= 15]\n",
    "                # Remove number strings that are greater than 15 digits\n",
    "            try:\n",
    "                for el in list(match):\n",
    "                    # Create a copy of the list since you're iterating over it\n",
    "                    if len(el.split('-')) > 3: continue # Year format YYYY-MM-DD\n",
    "                    for x in el.split(\"-\"):\n",
    "                        try:\n",
    "                            # Error catching is necessary because of possibility of stray non-number characters\n",
    "                            # if int(re.sub(r'\\D', '', x.strip())) in range(1900, 2100):\n",
    "                            if x.strip()[-4:].isdigit():\n",
    "                                if int(x.strip()[-4:]) in range(1900, 2100):\n",
    "                                    # Don't combine the two if statements to avoid a type conversion error\n",
    "                                    match.remove(el)\n",
    "                        except:\n",
    "                            pass\n",
    "            except:\n",
    "                pass\n",
    "            number = match\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        infoDict['phone'] = number\n",
    "\n",
    "        if debug:\n",
    "            print (\"\\n\", pprint(infoDict), \"\\n\")\n",
    "            code.interact(local=locals())\n",
    "        return number\n",
    "\n",
    "    def getName(self, inputString, infoDict, debug=False):\n",
    "        '''\n",
    "        Given an input string, returns possible matches for names. Uses regular expression based matching.\n",
    "        Needs an input string, a dictionary where values are being stored, and an optional parameter for debugging.\n",
    "        Modules required: time.perf_counter from time, code.\n",
    "        '''\n",
    "\n",
    "        # Reads Indian Names from the file, reduce all to lower case for easy comparision [Name lists]\n",
    "        indianNames = open(\"allNames.txt\", \"r\").read().lower()\n",
    "        # Lookup in a set is much faster\n",
    "        indianNames = set(indianNames.split())\n",
    "        \n",
    "\n",
    "        otherNameHits = []\n",
    "        nameHits = []\n",
    "        name = None\n",
    "\n",
    "        try:\n",
    "            # tokens, lines, sentences = self.preprocess(inputString)\n",
    "            tokens, lines, sentences = self.tokens, self.lines, self.sentences\n",
    "            # Try a regex chunk parser\n",
    "            # grammar = r'NAME: {<NN.*><NN.*>|<NN.*><NN.*><NN.*>}'\n",
    "            grammar = r'NAME: {<NN.*><NN.*><NN.*>*}'\n",
    "            # Noun phrase chunk is made out of two or three tags of type NN. (ie NN, NNP etc.) - typical of a name. {2,3} won't work, hence the syntax\n",
    "            # Note the correction to the rule. Change has been made later.\n",
    "            chunkParser = nltk.RegexpParser(grammar)\n",
    "            all_chunked_tokens = []\n",
    "            for tagged_tokens in lines:\n",
    "                # Creates a parse tree\n",
    "                if len(tagged_tokens) == 0: continue # Prevent it from printing warnings\n",
    "                chunked_tokens = chunkParser.parse(tagged_tokens)\n",
    "                all_chunked_tokens.append(chunked_tokens)\n",
    "                for subtree in chunked_tokens.subtrees():\n",
    "                    #  or subtree.label() == 'S' include in if condition if required\n",
    "                    if subtree.label() == 'NAME':\n",
    "                        for ind, leaf in enumerate(subtree.leaves()):\n",
    "                            if leaf[0].lower() in indianNames and 'NN' in leaf[1]:\n",
    "                                # Case insensitive matching, as indianNames have names in lowercase\n",
    "                                # Take only noun-tagged tokens\n",
    "                                # Surname is not in the name list, hence if match is achieved add all noun-type tokens\n",
    "                                # Pick upto 3 noun entities\n",
    "                                hit = \" \".join([el[0] for el in subtree.leaves()[ind:ind+3]])\n",
    "                                # Check for the presence of commas, colons, digits - usually markers of non-named entities \n",
    "                                if re.compile(r'[\\d,:]').search(hit): continue\n",
    "                                nameHits.append(hit)\n",
    "                                # Need to iterate through rest of the leaves because of possible mis-matches\n",
    "            # Going for the first name hit\n",
    "            if len(nameHits) > 0:\n",
    "                nameHits = [re.sub(r'[^a-zA-Z \\-]', '', el).strip() for el in nameHits] \n",
    "                name = \" \".join([el[0].upper()+el[1:].lower() for el in nameHits[0].split() if len(el)>0])\n",
    "                otherNameHits = nameHits[1:]\n",
    "\n",
    "        except Exception as e:\n",
    "            print (traceback.format_exc())\n",
    "            print (e)         \n",
    "\n",
    "        infoDict['name'] = name\n",
    "        infoDict['otherNameHits'] = otherNameHits\n",
    "\n",
    "        if debug:\n",
    "            print (\"\\n\", pprint(infoDict), \"\\n\")\n",
    "            code.interact(local=locals())\n",
    "        return name, otherNameHits  \n",
    "    \n",
    "    def getExperience(self,inputString,infoDict,debug=False):\n",
    "        experience=[]\n",
    "        try:\n",
    "            for sentence in self.lines:#find the index of the sentence where the degree is find and then analyse that sentence\n",
    "                    sen=\" \".join([words[0].lower() for words in sentence]) #string of words in sentence\n",
    "                    if re.search('experience',sen):\n",
    "                        sen_tokenised= nltk.word_tokenize(sen)\n",
    "                        tagged = nltk.pos_tag(sen_tokenised)\n",
    "                        entities = nltk.chunk.ne_chunk(tagged)\n",
    "                        for subtree in entities.subtrees():\n",
    "                            for leaf in subtree.leaves():\n",
    "                                if leaf[1]=='CD':\n",
    "                                    experience=leaf[0]\n",
    "        except Exception as e:\n",
    "            print (traceback.format_exc())\n",
    "            print (e) \n",
    "        if experience:\n",
    "            infoDict['experience'] = experience\n",
    "        else:\n",
    "            infoDict['experience']=0\n",
    "        if debug:\n",
    "            print (\"\\n\", pprint(infoDict), \"\\n\")\n",
    "            code.interact(local=locals())\n",
    "        return experience\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def getQualification(self,inputString,infoDict,D1,D2):\n",
    "        #key=list(qualification.keys())\n",
    "        qualification={'institute':'','year':''}\n",
    "        nameofinstitutes=open('nameofinstitutes.txt','r').read().lower()#open file which contains keywords like institutes,university usually  fond in institute names\n",
    "        nameofinstitues=set(nameofinstitutes.split())\n",
    "        instiregex=r'INSTI: {<DT.>?<NNP.*>+<IN.*>?<NNP.*>?}'\n",
    "        chunkParser = nltk.RegexpParser(instiregex)\n",
    "        \n",
    "        \n",
    "        try:           \n",
    "            index=[]\n",
    "            line=[]#saves all the lines where it finds the word of that education\n",
    "            for ind, sentence in enumerate(self.lines):#find the index of the sentence where the degree is find and then analyse that sentence\n",
    "                sen=\" \".join([words[0].lower() for words in sentence]) #string of words\n",
    "                if re.search(D1,sen) or re.search(D2,sen):\n",
    "                    index.append(ind)  #list of all indexes where word Ca lies\n",
    "            if index:#only finds for Ca rank and CA year if it finds the word Ca in the document\n",
    "                \n",
    "                for indextocheck in index:#checks all nearby lines where it founds the degree word.ex-'CA'\n",
    "                    for i in [indextocheck,indextocheck+1]: #checks the line with the keyword and just the next line to it\n",
    "                        try:\n",
    "                            try:\n",
    "                                wordstr=\" \".join(words[0] for words in self.lines[i])#string of that particular line\n",
    "                            except:\n",
    "                                wordstr=\"\"\n",
    "                            #if re.search(r'\\D\\d{1,3}\\D',wordstr.lower()) and qualification['rank']=='':\n",
    "                                    #qualification['rank']=re.findall(r'\\D\\d{1,3}\\D',wordstr.lower())\n",
    "                                    #line.append(wordstr)\n",
    "                            if re.search(r'\\b[21][09][8901][0-9]',wordstr.lower()) and qualification['year']=='':\n",
    "                                    qualification['year']=re.findall(r'\\b[21][09][8901][0-9]',wordstr.lower())\n",
    "                                    line.append(wordstr)\n",
    "                            chunked_line = chunkParser.parse(self.lines[i])#regex chunk for searching univ name\n",
    "                            for subtree in chunked_line.subtrees():\n",
    "                                    if subtree.label()=='INSTI':\n",
    "                                        for ind,leaves in enumerate(subtree):\n",
    "                                            if leaves[0].lower() in nameofinstitutes and leaves[1]=='NNP' and qualification['institute']=='':\n",
    "                                                qualification['institute']=' '.join([words[0]for words in subtree.leaves()])\n",
    "                                                line.append(wordstr)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print (traceback.format_exc())\n",
    "\n",
    "            if D1=='c\\.?a':\n",
    "                infoDict['%sinstitute'%D1] =\"I.C.A.I\"\n",
    "            else:\n",
    "                if qualification['institute']:\n",
    "                    infoDict['%sinstitute'%D1] = str(qualification['institute'])\n",
    "                else:\n",
    "                    infoDict['%sinstitute'%D1] = \"NULL\"\n",
    "            if qualification['year']:\n",
    "                infoDict['%syear'%D1] = int(qualification['year'][0])\n",
    "            else:\n",
    "                infoDict['%syear'%D1] =0\n",
    "            infoDict['%sline'%D1]=list(set(line))\n",
    "        except Exception as e:\n",
    "            print (traceback.format_exc())\n",
    "            print (e)\n",
    "\n",
    "\n",
    "    def Qualification(self,inputString,infoDict,debug=False):\n",
    "        degre=[]\n",
    "        #Q={'CAinformation':'','ICWAinformation':'','B.Cominformation':'','M.Cominformation':'','MBAinformation':''}\n",
    "        #degree=[]\n",
    "        #degree1=open('degree.txt','r').read().lower()#string to read from the txt file which contains all the degrees\n",
    "        #degree=set(el for el in degree1.split('\\n'))#saves all the degrees seperated by new lines,degree name contains both abbreviation and full names check file\n",
    "        #qualification1={'CAline':'','CAcollege':'','CArank':'','CAyear':''}\n",
    "        self.getQualification(self.inputString,infoDict,'c\\.?a','chartered accountant')\n",
    "        if infoDict['%sline'%'c\\.?a']:\n",
    "         degre.append('ca')\n",
    "        self.getQualification(self.inputString,infoDict,'icwa','icwa')\n",
    "        if infoDict['%sline'%'icwa']:\n",
    "         degre.append('icwa')\n",
    "        self.getQualification(self.inputString,infoDict,'b\\.?com','bachelor of commerce')\n",
    "        if infoDict['%sline'%'b\\.?com']:\n",
    "         degre.append('b.com')\n",
    "        self.getQualification(self.inputString,infoDict,'m\\.?com','masters of commerce')\n",
    "        if infoDict['%sline'%'m\\.?com']:\n",
    "         degre.append('m.com') \n",
    "        self.getQualification(self.inputString,infoDict,'mba','mba')\n",
    "        if infoDict['%sline'%'mba']:\n",
    "         degre.append('mba')\n",
    "        if degre:\n",
    "            infoDict['degree'] = degre\n",
    "        else:\n",
    "            infoDict['degree'] = \"NONE\"\n",
    "        if debug:\n",
    "            print (\"\\n\", pprint(infoDict), \"\\n\")\n",
    "            code.interact(local=locals())\n",
    "        return infoDict['degree']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verbose = False\n",
    "    if \"-v\" in str(sys.argv):\n",
    "        verbose = True\n",
    "    p = Parse(verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
